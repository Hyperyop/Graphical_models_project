{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\altegrad\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "import torch.nn.functional as F\n",
    "import torch_scatter as tc\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params = {\n",
    "        \"seed\": 41,\n",
    "        \"epochs\": 1000,\n",
    "        \"batch_size\": 128,\n",
    "        \"init_lr\": 0.001,\n",
    "        \"lr_reduce_factor\": 0.5,\n",
    "        \"lr_schedule_patience\": 10,\n",
    "        \"min_lr\": 1e-5,\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"print_epoch_interval\": 5,\n",
    "        \"max_time\": 12\n",
    "    }\n",
    "params_gcn = {\n",
    "    \"L\": 4,\n",
    "    \"hidden_dim\": 146,\n",
    "    \"out_dim\": 10,\n",
    "    \"residual\": True,\n",
    "    \"readout\": \"mean\",\n",
    "    \"in_feat_dropout\": 0.0,\n",
    "    \"dropout\": 0.0,\n",
    "    \"batch_norm\": True,\n",
    "    \"self_loop\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_dataset = pyg.datasets.GNNBenchmarkDataset(root='data/', name='CIFAR10')\n",
    "train_dataset = pyg.datasets.GNNBenchmarkDataset(root='data/', name='CIFAR10', split='train')\n",
    "test_dataset = pyg.datasets.GNNBenchmarkDataset(root='data/', name='CIFAR10', split='test')\n",
    "val_dataset = pyg.datasets.GNNBenchmarkDataset(root='data/', name='CIFAR10', split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=global_params[\"batch_size\"], shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=global_params[\"batch_size\"], shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=global_params[\"batch_size\"], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric as tg\n",
    "import torch.nn.functional as F\n",
    "import torch_scatter as tc\n",
    "import torch_scatter\n",
    "from enum import Enum\n",
    "from functools import partial\n",
    "\n",
    "class AggregationType(Enum):\n",
    "    ADD = 'add'\n",
    "    MEAN = 'mean'\n",
    "    MAX = 'max'\n",
    "    MIN = 'min'\n",
    "    MUL = 'mul'\n",
    "\n",
    "\n",
    "class messagePassing(torch.nn.Module):\n",
    "    def __init__(self, agg: AggregationType = AggregationType.ADD, **kwargs):\n",
    "        super(messagePassing, self).__init__()\n",
    "        self.agg = partial(torch_scatter.scatter, reduce = agg)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr = None, self_loop = False):\n",
    "        # first gather the values from the source nodes\n",
    "        temp = torch.index_select(x, 0, edge_index[0])\n",
    "        return self.agg(temp, edge_index[1], dim = 0, out = x) if self_loop else self.agg(temp, edge_index[1], dim = 0) \n",
    "\n",
    "class GCNconv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, add_self_loops = True,residual = True):\n",
    "        super(GCNconv, self).__init__()\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.mp = messagePassing(\"add\")\n",
    "        self.self_loops = add_self_loops\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        degrees = torch.sqrt(tg.utils.degree(edge_index[0]) +1 )\n",
    "        x = x / degrees.view(-1, 1)\n",
    "        x = self.mp(x, edge_index, self_loop = self.self_loops)\n",
    "        x = x / degrees.view(-1, 1)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "    \n",
    "class MLPReadout(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, L=2): #L=nb_hidden_layers\n",
    "        super().__init__()\n",
    "        list_FC_layers = [ torch.nn.Linear( input_dim//2**l , input_dim//2**(l+1) , bias=True ) for l in range(L) ]\n",
    "        list_FC_layers.append(torch.nn.Linear( input_dim//2**L , output_dim , bias=True ))\n",
    "        self.FC_layers = torch.nn.ModuleList(list_FC_layers)\n",
    "        self.L = L\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        for l in range(self.L):\n",
    "            y = self.FC_layers[l](y)\n",
    "            y = F.relu(y)\n",
    "        y = self.FC_layers[self.L](y)\n",
    "        return y\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels, num_layers, add_self_loops = True, reduce = \"mean\", n_iter = None, embedding = True, residual = True):\n",
    "        super(GCN, self).__init__()\n",
    "        if embedding:\n",
    "            self.embedding_layer = torch.nn.Embedding(in_channels, hidden_channels)\n",
    "        else:\n",
    "            self.embedding_layer = torch.nn.Linear(in_channels, hidden_channels)\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers-1):\n",
    "            self.convs.append(GCNconv(hidden_channels, hidden_channels, add_self_loops))\n",
    "        self.convs.append(GCNconv(hidden_channels, hidden_channels, add_self_loops))\n",
    "        self.residual = residual\n",
    "\n",
    "        # for now implementing sum reduction\n",
    "        match reduce:\n",
    "            case \"sum\":\n",
    "                self.reduce = partial(tc.scatter_add, dim = 0)\n",
    "            case \"mean\":\n",
    "                self.reduce = partial(tc.scatter_mean, dim = 0)\n",
    "            case \"max\":\n",
    "                self.reduce = partial(tc.scatter_max, dim = 0)\n",
    "            case \"set2set\":\n",
    "                self.reduce = set2setReadout(hidden_channels, n_iter)\n",
    "            case _:\n",
    "                raise ValueError(\"Invalid value for reduce\")\n",
    "        self.mlp = MLPReadout(hidden_channels, out_channels) if reduce != \"set2set\" else MLPReadout(hidden_channels*2, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, ptr=None):\n",
    "        x = self.embedding_layer(x)\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index)) + x if self.residual else F.relu(conv(x, edge_index))\n",
    "        x = self.reduce(x, ptr)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "class GATconv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, heads = 1, add_self_loops = True):\n",
    "        super(GATconv, self).__init__()\n",
    "        assert out_channels % heads == 0\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels, bias=False)\n",
    "        self.att_src = torch.nn.Parameter(torch.empty(1, heads, out_channels//heads))\n",
    "        self.att_dst = torch.nn.Parameter(torch.empty(1, heads, out_channels//heads))\n",
    "        self.heads = heads\n",
    "        self.self_loops = add_self_loops\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.lin.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.att_src)\n",
    "        torch.nn.init.xavier_uniform_(self.att_dst)\n",
    "\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        if self.self_loops:\n",
    "            edge_index, _ = tg.utils.add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        x = self.lin(x).view(x.size(0), self.heads, -1)\n",
    "        e_ij = self.prepareEdgeWeights(x, edge_index)\n",
    "        y = self.propagate(x, edge_index, e_ij)\n",
    "        return y.view(x.size(0), -1)\n",
    "        \n",
    "    def prepareEdgeWeights(self,x ,edge_index):\n",
    "        alpha_dst = (x * self.att_dst).sum(dim=-1)\n",
    "        alpha_src = (x * self.att_src).sum(dim=-1)\n",
    "        alpha = torch.index_select(alpha_src, 0, edge_index[0]) + torch.index_select(alpha_dst, 0, edge_index[1])\n",
    "        alpha = F.leaky_relu(alpha, 0.2)\n",
    "        e_ij = tg.utils.softmax(alpha, edge_index[1])\n",
    "        return e_ij\n",
    "    def propagate(self, x, edge_index, e_ij):\n",
    "        temp = torch.index_select(x, 0, edge_index[0])\n",
    "        temp = temp * e_ij.unsqueeze(-1)\n",
    "        return tc.scatter_add(temp, edge_index[1], dim=0)\n",
    "    \n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels, num_layers, heads, add_self_loops = True, reduce = \"mean\", n_iter = None, embedding = True):\n",
    "        super(GAT, self).__init__()\n",
    "        if embedding:\n",
    "            self.embedding_layer = torch.nn.Embedding(in_channels, hidden_channels)\n",
    "        else:\n",
    "            self.embedding_layer = torch.nn.linear(in_channels, hidden_channels)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers-1):\n",
    "            self.convs.append(GATconv(hidden_channels, hidden_channels, heads, add_self_loops))\n",
    "        self.convs.append(GATconv(hidden_channels, out_channels, heads, add_self_loops))\n",
    "        # for now implementing sum reduction\n",
    "        match reduce:\n",
    "            case \"sum\":\n",
    "                self.reduce = partial(tc.scatter_add, dim = 0)\n",
    "            case \"mean\":\n",
    "                self.reduce = partial(tc.scatter_mean, dim = 0)\n",
    "            case \"max\":\n",
    "                self.reduce = partial(tc.scatter_max, dim = 0)\n",
    "            case \"set2set\":\n",
    "                self.reduce = set2setReadout(out_channels, n_iter)\n",
    "            case _:\n",
    "                raise ValueError(\"Invalid value for reduce\")\n",
    "        self.mlp = MLPReadout(out_channels, 1) if reduce != \"set2set\" else MLPReadout(out_channels*2, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, ptr=None):\n",
    "        x = self.embedding_layer(x.view(-1))\n",
    "        a = 0\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "        x = self.reduce(x, ptr)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "class set2setReadout(torch.nn.Module):\n",
    "    def __init__(self, input_dim, n_iters, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = input_dim * 2\n",
    "        self.n_iters = n_iters\n",
    "        self.n_layers = n_layers\n",
    "        self.LSTM_layers = torch.nn.LSTM(self.output_dim, input_dim, num_layers=n_layers)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.LSTM_layers.reset_parameters()\n",
    "\n",
    "        \n",
    "    def forward(self, x, ptr):\n",
    "        batch_size = ptr.max().item() + 1\n",
    "        q = x.new_zeros((batch_size, self.output_dim)).to(x.device)\n",
    "        h = (x.new_zeros((self.n_layers, batch_size, self.input_dim)).to(x.device), x.new_zeros((self.n_layers, batch_size, self.input_dim)).to(x.device))\n",
    "        for _ in range(self.n_iters):\n",
    "            q, h = self.LSTM_layers(q.unsqueeze(0), h)\n",
    "            q = q.view(batch_size, self.input_dim)\n",
    "            e = (x * q[ptr]).sum(dim=-1, keepdim=True)\n",
    "            alpha = tg.utils.softmax(e, ptr, dim=-2)\n",
    "            r = tc.scatter_add(alpha * x, ptr, dim=-2)\n",
    "            q = torch.cat([q, r], dim=-1)\n",
    "        return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.classification import  MulticlassAccuracy\n",
    "def train_epoch(model, loss_fn, optimizer,scheduler, device, data_loader, epoch, metric):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_train_metric = 0\n",
    "    nb_data = 0\n",
    "    for iter, batch in enumerate(data_loader):\n",
    "        x = batch.x.to(device)\n",
    "        edge_index = batch.edge_index.to(device)\n",
    "        # edge_attr = batch.edge_attr.to(device)\n",
    "        targets = batch.y.to(device)\n",
    "        ptr = batch.batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        batch_scores = model.forward(x, edge_index, ptr)\n",
    "        loss = loss_fn(batch_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        epoch_train_metric += metric(F.sigmoid(batch_scores), targets).detach().item()\n",
    "        nb_data += targets.size(0)\n",
    "    epoch_loss /= (iter + 1)\n",
    "    epoch_train_metric /= (iter + 1)\n",
    "    \n",
    "    return epoch_loss, epoch_train_metric, optimizer\n",
    "def evaluate_epoch(model, loss_fn, device, data_loader, metric):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_metric = 0\n",
    "    nb_data = 0\n",
    "    with torch.no_grad():\n",
    "        for iter, batch in enumerate(data_loader):\n",
    "            x = batch.x.to(device)\n",
    "            edge_index = batch.edge_index.to(device)\n",
    "            # edge_attr = batch.edge_attr.to(device)\n",
    "            targets = batch.y.to(device)\n",
    "            ptr = batch.batch.to(device)\n",
    "            batch_scores = model.forward(x, edge_index, ptr)\n",
    "            loss = loss_fn(batch_scores, targets)\n",
    "            epoch_loss += loss.detach().item()\n",
    "            epoch_metric += metric(F.sigmoid(batch_scores), targets).item()\n",
    "            nb_data += targets.size(0)\n",
    "        epoch_loss /= (iter + 1)\n",
    "        epoch_metric /= (iter + 1)\n",
    "    return epoch_loss, epoch_metric\n",
    "def evaluate_model(model, loss_fn, device, data_loader):\n",
    "    epoch_loss, epoch_metric = evaluate_epoch(model, loss_fn, device, data_loader)\n",
    "    print(f'Loss: {epoch_loss}, metric: {epoch_metric}')\n",
    "    return epoch_loss, epoch_metric\n",
    "\n",
    "from collections import OrderedDict\n",
    "def train(model, loss_fn, optimizer, scheduler, device, train_loader, val_loader, test_loader, epochs, min_lr):\n",
    "    start = time.time()\n",
    "    logs = { 'train_loss': [], 'val_loss': [], 'test_loss': [], 'train_metric': [], 'val_metric': [], 'test_metric': [], \"time_per_epoch\": []}\n",
    "    metric =  MulticlassAccuracy(10).to(device)\n",
    "    with tqdm(range(epochs)) as pbar:\n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f'Epoch {epoch}')\n",
    "            epoch_loss, epoch_train_metric, optimizer = train_epoch(model, loss_fn, optimizer, scheduler, device, train_loader, epoch, metric = metric)\n",
    "            val_loss, val_metric = evaluate_epoch(model, loss_fn, device, val_loader, metric = metric)\n",
    "            test_loss, test_metric = evaluate_epoch(model, loss_fn, device, test_loader, metric = metric)\n",
    "            postfix_dict = OrderedDict(\n",
    "                time = time.time() - start,\n",
    "                lr=optimizer.param_groups[0]['lr'],\n",
    "                train_loss=epoch_loss,\n",
    "                val_loss=val_loss,\n",
    "                test_loss=test_loss,\n",
    "                train_metric=epoch_train_metric,\n",
    "                val_metric=val_metric,\n",
    "                test_metric=test_metric\n",
    "            )\n",
    "            pbar.set_postfix(postfix_dict)\n",
    "            scheduler.step(val_loss)\n",
    "            logs['train_loss'].append(epoch_loss)\n",
    "            logs['val_loss'].append(val_loss)\n",
    "            logs['test_loss'].append(test_loss)\n",
    "            logs['train_metric'].append(epoch_train_metric)\n",
    "            logs['val_metric'].append(val_metric)\n",
    "            logs['test_metric'].append(test_metric)\n",
    "            logs['time_per_epoch'].append(time.time() - start)\n",
    "            if optimizer.param_groups[0]['lr'] <= min_lr:\n",
    "                print('Early stopping')\n",
    "                break\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(3, params_gcn[\"out_dim\"], params_gcn[\"hidden_dim\"],params_gcn[\"L\"], embedding=False, residual=params_gcn[\"residual\"]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=global_params[\"init_lr\"], weight_decay=global_params[\"weight_decay\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=global_params[\"lr_reduce_factor\"],\n",
    "                                                       patience=global_params[\"lr_schedule_patience\"], min_lr=global_params[\"min_lr\"], verbose = True)\n",
    "loss = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (embedding_layer): Linear(in_features=3, out_features=146, bias=True)\n",
       "  (convs): ModuleList(\n",
       "    (0-3): 4 x GCNconv(\n",
       "      (lin): Linear(in_features=146, out_features=146, bias=True)\n",
       "      (mp): messagePassing()\n",
       "    )\n",
       "  )\n",
       "  (mlp): MLPReadout(\n",
       "    (FC_layers): ModuleList(\n",
       "      (0): Linear(in_features=146, out_features=73, bias=True)\n",
       "      (1): Linear(in_features=73, out_features=36, bias=True)\n",
       "      (2): Linear(in_features=36, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 202:  20%|██        | 202/1000 [36:30<2:20:06, 10.53s/it, time=2.19e+3, lr=0.001, train_loss=1.62, val_loss=1.67, test_loss=1.67, train_metric=0.421, val_metric=0.4, test_metric=0.395]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00202: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 224:  22%|██▏       | 224/1000 [40:32<2:26:48, 11.35s/it, time=2.43e+3, lr=0.0005, train_loss=1.59, val_loss=1.64, test_loss=1.66, train_metric=0.432, val_metric=0.412, test_metric=0.409]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00224: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 269:  27%|██▋       | 269/1000 [49:25<2:58:26, 14.65s/it, time=2.97e+3, lr=0.00025, train_loss=1.57, val_loss=1.62, test_loss=1.63, train_metric=0.44, val_metric=0.424, test_metric=0.421] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00269: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 293:  29%|██▉       | 293/1000 [54:00<2:05:36, 10.66s/it, time=3.24e+3, lr=0.000125, train_loss=1.55, val_loss=1.61, test_loss=1.62, train_metric=0.445, val_metric=0.42, test_metric=0.423] "
     ]
    }
   ],
   "source": [
    "logs = train(model,loss,  optimizer, scheduler, device, train_dataloader, val_dataloader, test_dataloader, global_params[\"epochs\"], global_params[\"min_lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (embedding_layer): Embedding(1, 145)\n",
       "  (convs): ModuleList(\n",
       "    (0-3): 4 x GCNconv(\n",
       "      (lin): Linear(in_features=145, out_features=145, bias=True)\n",
       "      (mp): messagePassing()\n",
       "    )\n",
       "  )\n",
       "  (mlp): MLPReadout(\n",
       "    (FC_layers): ModuleList(\n",
       "      (0): Linear(in_features=145, out_features=72, bias=True)\n",
       "      (1): Linear(in_features=72, out_features=36, bias=True)\n",
       "      (2): Linear(in_features=36, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_logs = logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_params =  {\n",
    "        \"L\": 4,\n",
    "        \"hidden_dim\": 144,\n",
    "        \"out_dim\": 144,\n",
    "        \"readout\": \"mean\",\n",
    "        \"n_heads\": 8,\n",
    "        \"in_feat_dropout\": 0.0,\n",
    "        \"dropout\": 0.0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_model = GAT(gat_params[\"hidden_dim\"], gat_params[\"hidden_dim\"], gat_params[\"out_dim\"], gat_params[\"L\"], gat_params[\"n_heads\"]).to(device)\n",
    "gat_optimizer = torch.optim.Adam(gat_model.parameters(), lr=global_params[\"init_lr\"], weight_decay=global_params[\"weight_decay\"])\n",
    "gat_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(gat_optimizer, mode='min', factor=global_params[\"lr_reduce_factor\"],\n",
    "                                                       patience=global_params[\"lr_schedule_patience\"], min_lr=global_params[\"min_lr\"], verbose = True)\n",
    "gat_loss = torch.nn.BCEWithLogitsLoss().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAT(\n",
       "  (embedding_layer): Embedding(1, 144)\n",
       "  (convs): ModuleList(\n",
       "    (0-3): 4 x GATconv(\n",
       "      (lin): Linear(in_features=144, out_features=144, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (mlp): MLPReadout(\n",
       "    (FC_layers): ModuleList(\n",
       "      (0): Linear(in_features=144, out_features=72, bias=True)\n",
       "      (1): Linear(in_features=72, out_features=36, bias=True)\n",
       "      (2): Linear(in_features=36, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24:   2%|▏         | 24/1000 [00:25<17:51,  1.10s/it, time=25.8, lr=0.001, train_loss=0.693, val_loss=0.692, test_loss=0.692, train_metric=0.502, val_metric=0.609, test_metric=0.609]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00024: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45:   4%|▍         | 45/1000 [00:47<16:52,  1.06s/it, time=47.9, lr=0.0005, train_loss=0.693, val_loss=0.693, test_loss=0.693, train_metric=0.502, val_metric=0.609, test_metric=0.609]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00045: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66:   7%|▋         | 66/1000 [01:09<16:24,  1.05s/it, time=69.9, lr=0.00025, train_loss=0.693, val_loss=0.693, test_loss=0.693, train_metric=0.498, val_metric=0.609, test_metric=0.609]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00066: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87:   9%|▊         | 87/1000 [01:31<15:57,  1.05s/it, time=91.6, lr=0.000125, train_loss=0.693, val_loss=0.693, test_loss=0.693, train_metric=0.499, val_metric=0.609, test_metric=0.609]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00087: reducing learning rate of group 0 to 6.2500e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 108:  11%|█         | 108/1000 [01:54<15:44,  1.06s/it, time=114, lr=6.25e-5, train_loss=0.693, val_loss=0.693, test_loss=0.693, train_metric=0.501, val_metric=0.609, test_metric=0.609]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00108: reducing learning rate of group 0 to 3.1250e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 129:  13%|█▎        | 129/1000 [02:16<15:44,  1.08s/it, time=137, lr=3.13e-5, train_loss=0.693, val_loss=0.693, test_loss=0.693, train_metric=0.504, val_metric=0.609, test_metric=0.609]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00129: reducing learning rate of group 0 to 1.5625e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 150:  15%|█▌        | 150/1000 [02:38<14:42,  1.04s/it, time=159, lr=1.56e-5, train_loss=0.693, val_loss=0.693, test_loss=0.693, train_metric=0.502, val_metric=0.609, test_metric=0.609]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00150: reducing learning rate of group 0 to 7.8125e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 171:  17%|█▋        | 171/1000 [03:02<16:09,  1.17s/it, time=182, lr=7.81e-6, train_loss=0.693, val_loss=0.693, test_loss=0.693, train_metric=0.498, val_metric=0.609, test_metric=0.609]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00171: reducing learning rate of group 0 to 3.9063e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 192:  19%|█▉        | 192/1000 [03:24<14:13,  1.06s/it, time=205, lr=3.91e-6, train_loss=0.693, val_loss=0.693, test_loss=0.693, train_metric=0.501, val_metric=0.609, test_metric=0.609]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00192: reducing learning rate of group 0 to 1.9531e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 212:  21%|██        | 212/1000 [03:47<14:05,  1.07s/it, time=227, lr=1.95e-6, train_loss=0.693, val_loss=0.693, test_loss=0.693, train_metric=0.502, val_metric=0.609, test_metric=0.609]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00213: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gat_logs = train(gat_model, gat_loss, gat_optimizer, gat_scheduler, device, train_dataloader, val_dataloader, test_dataloader, global_params[\"epochs\"], global_params[\"min_lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_model_with_set2set = GAT(gat_params[\"hidden_dim\"], gat_params[\"hidden_dim\"], gat_params[\"out_dim\"], gat_params[\"L\"], gat_params[\"n_heads\"], reduce=\"set2set\", n_iter= 10).to(device)\n",
    "gat_optimizer_with_set2set = torch.optim.Adam(gat_model_with_set2set.parameters(), lr=global_params[\"init_lr\"], weight_decay=global_params[\"weight_decay\"])\n",
    "gat_scheduler_with_set2set = torch.optim.lr_scheduler.ReduceLROnPlateau(gat_optimizer_with_set2set, mode='min', factor=global_params[\"lr_reduce_factor\"],\n",
    "                                                         patience=global_params[\"lr_schedule_patience\"], min_lr=global_params[\"min_lr\"], verbose = True)\n",
    "gat_loss_with_set2set = torch.nn.BCEWithLogitsLoss().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29:   3%|▎         | 29/1000 [00:59<33:43,  2.08s/it, time=59.4, lr=0.001, train_loss=0.693, val_loss=0.693, test_loss=0.693, train_metric=0.471, val_metric=0.609, test_metric=0.609]"
     ]
    }
   ],
   "source": [
    "set2set_logs = train(gat_model_with_set2set, gat_loss_with_set2set, gat_optimizer_with_set2set, gat_scheduler_with_set2set, device, train_dataloader, val_dataloader, test_dataloader, global_params[\"epochs\"], global_params[\"min_lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altegrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
