{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric as tg\n",
    "import torch.nn.functional as F\n",
    "import torch_scatter as tc\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tg.datasets.ZINC('./data/ZINC', subset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = A.x\n",
    "edge_index = A.edge_index\n",
    "edge_attr = A.edge_attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mp import messagePassing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = messagePassing(\"add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNconv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, add_self_loops = True,):\n",
    "        super(GCNconv, self).__init__()\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.mp = messagePassing(\"add\")\n",
    "        self.self_loops = add_self_loops\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        degrees = torch.sqrt(tg.utils.degree(edge_index[0]) +1 )\n",
    "        x = x / degrees.view(-1, 1)\n",
    "        x = self.mp(x, edge_index, self_loop = self.self_loops)\n",
    "        x = x / degrees.view(-1, 1)\n",
    "        x = self.lin(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPReadout(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, L=2): #L=nb_hidden_layers\n",
    "        super().__init__()\n",
    "        list_FC_layers = [ torch.nn.Linear( input_dim//2**l , input_dim//2**(l+1) , bias=True ) for l in range(L) ]\n",
    "        list_FC_layers.append(torch.nn.Linear( input_dim//2**L , output_dim , bias=True ))\n",
    "        self.FC_layers = torch.nn.ModuleList(list_FC_layers)\n",
    "        self.L = L\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        for l in range(self.L):\n",
    "            y = self.FC_layers[l](y)\n",
    "            y = F.relu(y)\n",
    "        y = self.FC_layers[self.L](y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels, num_layers, add_self_loops=True, reduce=\"mean\", residual=True):\n",
    "        super(GCN, self).__init__()\n",
    "        self.embedding_layer = torch.nn.Embedding(28, in_channels)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            in_channels = in_channels if i == 0 else hidden_channels\n",
    "            self.convs.append(GCNconv(in_channels, hidden_channels, add_self_loops))\n",
    "        self.convs.append(GCNconv(hidden_channels, out_channels, add_self_loops))\n",
    "\n",
    "        # for now implementing sum reduction\n",
    "        if reduce == \"sum\":\n",
    "            self.reduce = partial(tc.scatter_add, dim=0)\n",
    "        elif reduce == \"mean\":\n",
    "            self.reduce = partial(tc.scatter_mean, dim=0)\n",
    "        elif reduce == \"max\":\n",
    "            self.reduce = partial(tc.scatter_max, dim=0)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for reduce\")\n",
    "\n",
    "        self.mlp = MLPReadout(out_channels, 1)\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, x, edge_index, ptr=None):\n",
    "        x = self.embedding_layer(x.view(-1))\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index)) + x if self.residual else F.relu(conv(x, edge_index))\n",
    "        x = self.reduce(x, ptr)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=41; epochs=1000; batch_size=128; init_lr=1e-3; lr_reduce_factor=0.5; lr_schedule_patience=10; min_lr = 1e-5; weight_decay=0\n",
    "L=4; hidden_dim=145; out_dim=hidden_dim; dropout=0.0; readout='mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Create loaders for train, validation, and test sets\n",
    "train_loader = DataLoader(dataset[:8000], batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset[8000:9000], batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset[9000:], batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = lambda x,y: F.l1_loss(x,y).detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, loss_fn, optimizer,scheduler, device, data_loader, epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_train_mae = 0\n",
    "    nb_data = 0\n",
    "    for iter, batch in enumerate(data_loader):\n",
    "        x = batch.x.to(device)\n",
    "        edge_index = batch.edge_index.to(device)\n",
    "        # edge_attr = batch.edge_attr.to(device)\n",
    "        targets = batch.y.to(device)\n",
    "        ptr = batch.batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        batch_scores = model.forward(x, edge_index, ptr)\n",
    "        loss = loss_fn(batch_scores, targets.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        epoch_train_mae += mae(batch_scores, targets.view(-1, 1))\n",
    "        nb_data += targets.size(0)\n",
    "    epoch_loss /= (iter + 1)\n",
    "    epoch_train_mae /= (iter + 1)\n",
    "    \n",
    "    return epoch_loss, epoch_train_mae, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(model, loss_fn, device, data_loader):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_mae = 0\n",
    "    nb_data = 0\n",
    "    with torch.no_grad():\n",
    "        for iter, batch in enumerate(data_loader):\n",
    "            x = batch.x.to(device)\n",
    "            edge_index = batch.edge_index.to(device)\n",
    "            # edge_attr = batch.edge_attr.to(device)\n",
    "            targets = batch.y.to(device)\n",
    "            ptr = batch.batch.to(device)\n",
    "            batch_scores = model.forward(x, edge_index, ptr)\n",
    "            loss = loss_fn(batch_scores, targets.view(-1, 1))\n",
    "            epoch_loss += loss.detach().item()\n",
    "            epoch_mae += mae(batch_scores, targets.view(-1, 1))\n",
    "            nb_data += targets.size(0)\n",
    "        epoch_loss /= (iter + 1)\n",
    "        epoch_mae /= (iter + 1)\n",
    "    return epoch_loss, epoch_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loss_fn, device, data_loader):\n",
    "    epoch_loss, epoch_mae = evaluate_epoch(model, loss_fn, device, data_loader)\n",
    "    print(f'Loss: {epoch_loss}, MAE: {epoch_mae}')\n",
    "    return epoch_loss, epoch_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import OrderedDict\n",
    "def train(model, loss_fn, optimizer, scheduler, device, train_loader, val_loader, test_loader, epochs):\n",
    "    start = time.time()\n",
    "    logs = { 'train_loss': [], 'val_loss': [], 'test_loss': [], 'train_mae': [], 'val_mae': [], 'test_mae': [], \"time_per_epoch\": []}\n",
    "    with tqdm(range(epochs)) as pbar:\n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f'Epoch {epoch}')\n",
    "            epoch_loss, epoch_train_mae, optimizer = train_epoch(model, loss_fn, optimizer, scheduler, device, train_loader, epoch)\n",
    "            val_loss, val_mae = evaluate_epoch(model, loss_fn, device, val_loader)\n",
    "            test_loss, test_mae = evaluate_epoch(model, loss_fn, device, test_loader)\n",
    "            postfix_dict = OrderedDict(\n",
    "                time = time.time() - start,\n",
    "                lr=optimizer.param_groups[0]['lr'],\n",
    "                train_loss=epoch_loss,\n",
    "                val_loss=val_loss,\n",
    "                test_loss=test_loss,\n",
    "                train_mae=epoch_train_mae,\n",
    "                val_mae=val_mae,\n",
    "                test_mae=test_mae\n",
    "            )\n",
    "            pbar.set_postfix(postfix_dict)\n",
    "            scheduler.step(val_loss)\n",
    "            logs['train_loss'].append(epoch_loss)\n",
    "            logs['val_loss'].append(val_loss)\n",
    "            logs['test_loss'].append(test_loss)\n",
    "            logs['train_mae'].append(epoch_train_mae)\n",
    "            logs['val_mae'].append(val_mae)\n",
    "            logs['test_mae'].append(test_mae)\n",
    "            logs['time_per_epoch'].append(time.time() - start)\n",
    "            if optimizer.param_groups[0]['lr'] <= min_lr:\n",
    "                print('Early stopping')\n",
    "                break\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(in_channels=hidden_dim, hidden_channels=hidden_dim, out_channels=out_dim, num_layers=L, reduce=readout).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=init_lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_reduce_factor, patience=lr_schedule_patience, min_lr=min_lr,verbose=True)\n",
    "loss = torch.nn.L1Loss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 236:  24%|██▎       | 236/1000 [04:28<14:27,  1.14s/it, time=268, lr=1.56e-5, train_loss=0.411, val_loss=0.47, test_loss=0.434, train_mae=0.411, val_mae=0.47, test_mae=0.434]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 225:  22%|██▎       | 225/1000 [04:26<15:19,  1.19s/it, time=267, lr=1.56e-5, train_loss=0.405, val_loss=0.472, test_loss=0.425, train_mae=0.405, val_mae=0.472, test_mae=0.425] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 242:  24%|██▍       | 242/1000 [05:07<16:02,  1.27s/it, time=307, lr=1.56e-5, train_loss=0.386, val_loss=0.468, test_loss=0.431, train_mae=0.386, val_mae=0.468, test_mae=0.431] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 241:  24%|██▍       | 241/1000 [05:30<17:20,  1.37s/it, time=330, lr=1.56e-5, train_loss=0.366, val_loss=0.46, test_loss=0.417, train_mae=0.366, val_mae=0.46, test_mae=0.417]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 219:  22%|██▏       | 219/1000 [05:22<19:08,  1.47s/it, time=322, lr=1.56e-5, train_loss=0.411, val_loss=0.476, test_loss=0.43, train_mae=0.411, val_mae=0.476, test_mae=0.43]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "GCN_logs = []\n",
    "for _ in range(5):\n",
    "    model = GCN(in_channels=hidden_dim, hidden_channels=hidden_dim, out_channels=out_dim, num_layers=L, reduce=readout).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=init_lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_reduce_factor, patience=lr_schedule_patience, min_lr=min_lr,verbose=True)\n",
    "    loss = torch.nn.L1Loss().to(device)\n",
    "    GCN_logs.append(train(model, loss, optimizer, scheduler, device, train_loader, val_loader, test_loader, epochs  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATconv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, heads = 1, add_self_loops = True, f_additive = True):\n",
    "        super(GATconv, self).__init__()\n",
    "        # print( out_channels, heads, out_channels//heads)\n",
    "        assert out_channels % heads == 0\n",
    "        \n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels, bias=False)\n",
    "        self.att_src = torch.nn.Parameter(torch.empty(1, heads, out_channels//heads))\n",
    "        self.att_dst = torch.nn.Parameter(torch.empty(1, heads, out_channels//heads))\n",
    "        self.heads = heads\n",
    "        self.self_loops = add_self_loops\n",
    "        self.reset_parameters()\n",
    "        self.f_additive = f_additive\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.lin.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.att_src)\n",
    "        torch.nn.init.xavier_uniform_(self.att_dst)\n",
    "\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        if self.self_loops:\n",
    "            edge_index, _ = tg.utils.add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        x = self.lin(x).view(x.size(0), self.heads, -1)\n",
    "        e_ij = self.prepareEdgeWeights(x, edge_index)\n",
    "        y = self.propagate(x, edge_index, e_ij)\n",
    "        return y.view(x.size(0), -1)\n",
    "        \n",
    "    def prepareEdgeWeights(self,x ,edge_index):\n",
    "        alpha_dst = (x * self.att_dst).sum(dim=-1)\n",
    "        alpha_src = (x * self.att_src).sum(dim=-1)\n",
    "        alpha = torch.index_select(alpha_src, 0, edge_index[0]) + torch.index_select(alpha_dst, 0, edge_index[1])\n",
    "        alpha = F.leaky_relu(alpha, 0.2)\n",
    "        e_ij = tg.utils.softmax(alpha, edge_index[1])\n",
    "        return e_ij\n",
    "    def propagate(self, x, edge_index, e_ij):\n",
    "        temp = torch.index_select(x, 0, edge_index[0])\n",
    "        edge_weights = e_ij.unsqueeze(-1) + 1 if self.f_additive else e_ij.unsqueeze(-1)\n",
    "        temp = temp * edge_weights\n",
    "        return tc.scatter_add(temp, edge_index[1], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels, num_layers, heads, add_self_loops = True, reduce = \"mean\", residual = True, f_additive = True):\n",
    "        super(GAT, self).__init__()\n",
    "        self.embedding_layer = torch.nn.Embedding(28, in_channels)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers-1):\n",
    "            in_channels = in_channels if i == 0 else hidden_channels\n",
    "            self.convs.append(GATconv(in_channels, hidden_channels, heads, add_self_loops, f_additive))\n",
    "        self.convs.append(GATconv(hidden_channels, out_channels, heads, add_self_loops, f_additive))\n",
    "        # for now implementing sum reduction\n",
    "        if reduce == \"sum\":\n",
    "            self.reduce = partial(tc.scatter_add, dim=0)\n",
    "        elif reduce == \"mean\":\n",
    "            self.reduce = partial(tc.scatter_mean, dim=0)\n",
    "        elif reduce == \"max\":\n",
    "            self.reduce = partial(tc.scatter_max, dim=0)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for reduce\")\n",
    "        self.mlp = MLPReadout(out_channels, 1)\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, x, edge_index, ptr=None):\n",
    "        x = self.embedding_layer(x.view(-1))\n",
    "        a = 0\n",
    "        for conv in self.convs:\n",
    "            x = F.elu(conv(x, edge_index))\n",
    "        x = self.reduce(x, ptr)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params =  {\n",
    "        \"L\": 4,\n",
    "        \"hidden_dim\": 18,\n",
    "        \"out_dim\": 144,\n",
    "        \"residual\": True,\n",
    "        \"readout\": \"mean\",\n",
    "        \"n_heads\": 8,\n",
    "        \"in_feat_dropout\": 0.0,\n",
    "        \"dropout\": 0.0,\n",
    "        \"batch_norm\": True,\n",
    "        \"self_loop\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dim = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAT_model = GAT(in_channels=hidden_dim, hidden_channels=hidden_dim, out_channels=out_dim, num_layers=L, heads=2, reduce=readout).to(device)\n",
    "optimizer = torch.optim.Adam(GAT_model.parameters(), lr=init_lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_reduce_factor, patience=lr_schedule_patience, min_lr=min_lr,verbose=True)\n",
    "loss = torch.nn.L1Loss().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 177:  18%|█▊        | 177/1000 [05:51<27:13,  1.99s/it, time=351, lr=1.56e-5, train_loss=0.239, val_loss=0.421, test_loss=0.41, train_mae=0.239, val_mae=0.421, test_mae=0.41]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 153:  15%|█▌        | 153/1000 [05:06<28:17,  2.00s/it, time=307, lr=1.56e-5, train_loss=0.275, val_loss=0.432, test_loss=0.402, train_mae=0.275, val_mae=0.432, test_mae=0.402] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 163:  16%|█▋        | 163/1000 [05:20<27:27,  1.97s/it, time=321, lr=1.56e-5, train_loss=0.268, val_loss=0.434, test_loss=0.402, train_mae=0.268, val_mae=0.434, test_mae=0.402] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 165:  16%|█▋        | 165/1000 [02:10<11:02,  1.26it/s, time=131, lr=1.56e-5, train_loss=0.244, val_loss=0.426, test_loss=0.393, train_mae=0.244, val_mae=0.426, test_mae=0.393]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 159:  16%|█▌        | 159/1000 [02:05<11:06,  1.26it/s, time=126, lr=1.56e-5, train_loss=0.26, val_loss=0.425, test_loss=0.402, train_mae=0.26, val_mae=0.425, test_mae=0.402]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gat_logs = []\n",
    "for _ in range(5):\n",
    "    GAT_model = GAT(in_channels=hidden_dim, hidden_channels=hidden_dim, out_channels=out_dim, num_layers=L, heads=8, reduce=readout).to(device)\n",
    "    optimizer = torch.optim.Adam(GAT_model.parameters(), lr=init_lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_reduce_factor, patience=lr_schedule_patience, min_lr=min_lr,verbose=True)\n",
    "    loss = torch.nn.L1Loss().to(device)\n",
    "    gat_logs.append(train(GAT_model, loss, optimizer, scheduler, device, train_loader, val_loader, test_loader, epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class set2setReadout(torch.nn.Module):\n",
    "    def __init__(self, input_dim, n_iters, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = input_dim * 2\n",
    "        self.n_iters = n_iters\n",
    "        self.n_layers = n_layers\n",
    "        self.LSTM_layers = torch.nn.LSTM(self.output_dim, input_dim, num_layers=n_layers)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.LSTM_layers.reset_parameters()\n",
    "\n",
    "        \n",
    "    def forward(self, x, ptr):\n",
    "        batch_size = ptr.max().item() + 1\n",
    "        q = x.new_zeros((batch_size, self.output_dim)).to(x.device)\n",
    "        h = (x.new_zeros((self.n_layers, batch_size, self.input_dim)).to(x.device), x.new_zeros((self.n_layers, batch_size, self.input_dim)).to(x.device))\n",
    "        for _ in range(self.n_iters):\n",
    "            q, h = self.LSTM_layers(q.unsqueeze(0), h)\n",
    "            q = q.view(batch_size, self.input_dim)\n",
    "            e = (x * q[ptr]).sum(dim=-1, keepdim=True)\n",
    "            alpha = tg.utils.softmax(e, ptr, dim=-2)\n",
    "            r = tc.scatter_add(alpha * x, ptr, dim=-2)\n",
    "            q = torch.cat([q, r], dim=-1)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT_set2set(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels, num_layers, heads, add_self_loops = True, reduce = \"mean\", n_iters=5, f_additive = True):\n",
    "        super(GAT_set2set, self).__init__()\n",
    "        self.embedding_layer = torch.nn.Embedding(28, in_channels)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers-1):\n",
    "            in_channels = in_channels if i == 0 else hidden_channels\n",
    "            self.convs.append(GATconv(in_channels, hidden_channels, heads, add_self_loops, f_additive))\n",
    "        self.convs.append(GATconv(hidden_channels, out_channels//2, heads, add_self_loops, f_additive))\n",
    "        self.set2set = set2setReadout(out_channels//2, n_iters)\n",
    "        self.mlp = MLPReadout(out_channels , 1,1)\n",
    "\n",
    "    def forward(self, x, edge_index, ptr=None):\n",
    "        x = self.embedding_layer(x.view(-1))\n",
    "        for conv in self.convs:\n",
    "            x = F.elu(conv(x, edge_index))\n",
    "        x = self.set2set(x, ptr)\n",
    "        x = F.relu(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAT_model_set2set = GAT_set2set(in_channels=hidden_dim, hidden_channels=hidden_dim//2, out_channels=out_dim, num_layers=L, heads=8, reduce=readout, n_iters= 10).to(device)\n",
    "optimizer = torch.optim.Adam(GAT_model_set2set.parameters(), lr=init_lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_reduce_factor, patience=lr_schedule_patience, min_lr=min_lr,verbose=True)\n",
    "loss = torch.nn.L1Loss().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 158:  16%|█▌        | 158/1000 [04:05<21:47,  1.55s/it, time=245, lr=1.56e-5, train_loss=0.952, val_loss=0.937, test_loss=0.957, train_mae=0.952, val_mae=0.937, test_mae=0.957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 134:  13%|█▎        | 134/1000 [03:27<22:21,  1.55s/it, time=208, lr=1.56e-5, train_loss=0.962, val_loss=0.962, test_loss=0.963, train_mae=0.962, val_mae=0.962, test_mae=0.963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 112:  11%|█         | 112/1000 [02:52<22:49,  1.54s/it, time=173, lr=1.56e-5, train_loss=1.1, val_loss=1.13, test_loss=1.05, train_mae=1.1, val_mae=1.13, test_mae=1.05]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 165:  16%|█▋        | 165/1000 [04:14<21:25,  1.54s/it, time=254, lr=1.56e-5, train_loss=0.903, val_loss=0.902, test_loss=0.895, train_mae=0.903, val_mae=0.902, test_mae=0.895] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 256:  26%|██▌       | 256/1000 [06:36<19:10,  1.55s/it, time=396, lr=1.56e-5, train_loss=0.597, val_loss=0.645, test_loss=0.598, train_mae=0.597, val_mae=0.645, test_mae=0.598] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gat_logs_set2set =[]\n",
    "for _ in range(5):\n",
    "    GAT_model_set2set = GAT_set2set(in_channels=hidden_dim, hidden_channels=hidden_dim//2, out_channels=out_dim, num_layers=L, heads=8, reduce=readout, n_iters= 10).to(device)\n",
    "    optimizer = torch.optim.Adam(GAT_model_set2set.parameters(), lr=init_lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_reduce_factor, patience=lr_schedule_patience, min_lr=min_lr,verbose=True)\n",
    "    loss = torch.nn.L1Loss().to(device)\n",
    "\n",
    "    gat_logs_set2set.append(train(GAT_model_set2set, loss, optimizer, scheduler, device, train_loader, val_loader, test_loader, epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model, GAT_model, GAT_model_set2set]\n",
    "logs = [GCN_logs, gat_logs, gat_logs_set2set]\n",
    "parameter_counts = [count_parameters(model) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101917, 101233, 103825]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "pickle.dump(logs, open(\"logs_zinc_additive.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altegrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
